{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyORfr9KpBPAo1VArvxfKZef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaPNouri/BERTEmbedding/blob/main/BertEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an optimized version of BERT which uses the BERT embedding instead of Bert classification."
      ],
      "metadata": {
        "id": "IVQM9z9jfBus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR2VBANWfTPe",
        "outputId": "3b5a4d8d-9d40-46b1-d74f-c80bc25eab69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KWvO9dJigbOf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DATA = 500\n",
        "MAX_SENTENCE_LENGTH = 500"
      ],
      "metadata": {
        "id": "5Y3I05aXos02"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset as a zip file from the git repo\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://raw.githubusercontent.com/AlirezaPNouri/BERTEmbedding/main/5KArticles.csv'\n",
        "res = requests.get(url, allow_redirects=True)\n",
        "with open('small_dataset_NYT.csv','wb') as file:\n",
        "    file.write(res.content)\n",
        "print('Download is done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYN-I2gDfol5",
        "outputId": "4f4d428f-31d4-4164-b94e-619d7ccf10d3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download is done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/small_dataset_NYT.csv\", header= None, skiprows=1)\n",
        "print('The original size of dataset is {}'.format(df.shape))\n",
        "df = df.dropna(how='any', axis=0)\n",
        "\n",
        "df = df[0:MAX_DATA][[1, 2]]\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences extracted from dataset is {:,}\\n'.format(df.shape[0]))\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)\n",
        "\n",
        "df.columns = ['id', 'content']\n",
        "df['content'] = df['content'].apply(lambda x: x[:MAX_SENTENCE_LENGTH])\n",
        "print('Columns are : {}'.format([name for name in df.columns]))\n",
        "print('The dimension of the dataset is {}'.format(df.shape))\n",
        "print(df.sample)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oWEaHJpofF3",
        "outputId": "f2ba4d52-6ac0-41c8-a3da-ea4929469289"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original size of dataset is (5001, 3)\n",
            "Number of training sentences extracted from dataset is 500\n",
            "\n",
            "Columns are : ['id', 'content']\n",
            "The dimension of the dataset is (500, 2)\n",
            "<bound method NDFrame.sample of                    id                                            content\n",
            "0    19900101_0000000  HURRYING through the tunnel at Veterans Stadiu...\n",
            "1    19900101_0000001  FOR six years, American corporations have been...\n",
            "2    19900101_0000002  Alcide Chaisson, who is 69 years old and lives...\n",
            "3    19900101_0000003  Gary Anderson is the third-most accurate kicke...\n",
            "4    19900101_0000004  The National Bank of Poland is devaluing the P...\n",
            "..                ...                                                ...\n",
            "495  19900106_0000495  Turner Broadcasting System said Thursday that ...\n",
            "496  19900106_0000496  Aaron Pryor, the former world junior welterwei...\n",
            "497  19900106_0000497  On Wednesday, when Dipendra Bir Bikram Shah De...\n",
            "498  19900106_0000498  At the age of 58, after making the playoffs 11...\n",
            "499  19900106_0000499  Terry Dehere of Seton Hall played his high sch...\n",
            "\n",
            "[500 rows x 2 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x : ' '.join(x.split()[:MAX_SENTENCE_LENGTH]))"
      ],
      "metadata": {
        "id": "3pAXS3GSs-DU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "# input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\"))[None, :]  # Batch size 1\n",
        "# outputs = model(input_ids)\n",
        "# last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "\n",
        "# print(tokenizer.tokenize(\"Hello, my dog is cute\"))\n",
        "# print(last_hidden_states.shape) # the first and the last ones are the cls and sep\n",
        "Embedding_dict = {}\n",
        "for id, doc in zip(range(MAX_DATA),df['content']):\n",
        "  Embedding_dict[id] = model(tf.constant(tokenizer.encode(doc))[None, :])[0]\n",
        "  print('Document {} is done!'.format(id))\n",
        "# print(Embedding_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "05RgrBqIfTYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(df['content'][doc]))\n",
        "\n",
        "print(Embedding_dict[doc][1:-1])\n",
        "temp_list = Embedding_dict[doc][0]\n",
        "newList = np.array(temp_list)[0]\n",
        "print(newList)"
      ],
      "metadata": {
        "id": "GEpU8MM-wZVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Embedding_dict[6].shape)\n",
        "doc_word_embedding = {}\n",
        "for doc in range(MAX_DATA):\n",
        "  tmp_list = list()\n",
        "  temp_list = ()\n",
        "  temp_list = np.array(Embedding_dict[doc][0])\n",
        "  tmp_list.append(('cls', temp_list[0]))\n",
        "  for word, embedding in zip(tokenizer.tokenize(df['content'][doc]), temp_list[1:-1]):\n",
        "    tmp_list.append((word,embedding))\n",
        "  tmp_list.append(('sep', temp_list[-1]))\n",
        "  doc_word_embedding[doc] = tmp_list\n",
        "  print('doc {} is done!'.format(doc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DXORpw7qjr0",
        "outputId": "f39b5ae3-5e06-4b2d-8fcd-6a18f625131d"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 115, 768)\n",
            "doc 0 is done!\n",
            "doc 1 is done!\n",
            "doc 2 is done!\n",
            "doc 3 is done!\n",
            "doc 4 is done!\n",
            "doc 5 is done!\n",
            "doc 6 is done!\n",
            "doc 7 is done!\n",
            "doc 8 is done!\n",
            "doc 9 is done!\n",
            "doc 10 is done!\n",
            "doc 11 is done!\n",
            "doc 12 is done!\n",
            "doc 13 is done!\n",
            "doc 14 is done!\n",
            "doc 15 is done!\n",
            "doc 16 is done!\n",
            "doc 17 is done!\n",
            "doc 18 is done!\n",
            "doc 19 is done!\n",
            "doc 20 is done!\n",
            "doc 21 is done!\n",
            "doc 22 is done!\n",
            "doc 23 is done!\n",
            "doc 24 is done!\n",
            "doc 25 is done!\n",
            "doc 26 is done!\n",
            "doc 27 is done!\n",
            "doc 28 is done!\n",
            "doc 29 is done!\n",
            "doc 30 is done!\n",
            "doc 31 is done!\n",
            "doc 32 is done!\n",
            "doc 33 is done!\n",
            "doc 34 is done!\n",
            "doc 35 is done!\n",
            "doc 36 is done!\n",
            "doc 37 is done!\n",
            "doc 38 is done!\n",
            "doc 39 is done!\n",
            "doc 40 is done!\n",
            "doc 41 is done!\n",
            "doc 42 is done!\n",
            "doc 43 is done!\n",
            "doc 44 is done!\n",
            "doc 45 is done!\n",
            "doc 46 is done!\n",
            "doc 47 is done!\n",
            "doc 48 is done!\n",
            "doc 49 is done!\n",
            "doc 50 is done!\n",
            "doc 51 is done!\n",
            "doc 52 is done!\n",
            "doc 53 is done!\n",
            "doc 54 is done!\n",
            "doc 55 is done!\n",
            "doc 56 is done!\n",
            "doc 57 is done!\n",
            "doc 58 is done!\n",
            "doc 59 is done!\n",
            "doc 60 is done!\n",
            "doc 61 is done!\n",
            "doc 62 is done!\n",
            "doc 63 is done!\n",
            "doc 64 is done!\n",
            "doc 65 is done!\n",
            "doc 66 is done!\n",
            "doc 67 is done!\n",
            "doc 68 is done!\n",
            "doc 69 is done!\n",
            "doc 70 is done!\n",
            "doc 71 is done!\n",
            "doc 72 is done!\n",
            "doc 73 is done!\n",
            "doc 74 is done!\n",
            "doc 75 is done!\n",
            "doc 76 is done!\n",
            "doc 77 is done!\n",
            "doc 78 is done!\n",
            "doc 79 is done!\n",
            "doc 80 is done!\n",
            "doc 81 is done!\n",
            "doc 82 is done!\n",
            "doc 83 is done!\n",
            "doc 84 is done!\n",
            "doc 85 is done!\n",
            "doc 86 is done!\n",
            "doc 87 is done!\n",
            "doc 88 is done!\n",
            "doc 89 is done!\n",
            "doc 90 is done!\n",
            "doc 91 is done!\n",
            "doc 92 is done!\n",
            "doc 93 is done!\n",
            "doc 94 is done!\n",
            "doc 95 is done!\n",
            "doc 96 is done!\n",
            "doc 97 is done!\n",
            "doc 98 is done!\n",
            "doc 99 is done!\n",
            "doc 100 is done!\n",
            "doc 101 is done!\n",
            "doc 102 is done!\n",
            "doc 103 is done!\n",
            "doc 104 is done!\n",
            "doc 105 is done!\n",
            "doc 106 is done!\n",
            "doc 107 is done!\n",
            "doc 108 is done!\n",
            "doc 109 is done!\n",
            "doc 110 is done!\n",
            "doc 111 is done!\n",
            "doc 112 is done!\n",
            "doc 113 is done!\n",
            "doc 114 is done!\n",
            "doc 115 is done!\n",
            "doc 116 is done!\n",
            "doc 117 is done!\n",
            "doc 118 is done!\n",
            "doc 119 is done!\n",
            "doc 120 is done!\n",
            "doc 121 is done!\n",
            "doc 122 is done!\n",
            "doc 123 is done!\n",
            "doc 124 is done!\n",
            "doc 125 is done!\n",
            "doc 126 is done!\n",
            "doc 127 is done!\n",
            "doc 128 is done!\n",
            "doc 129 is done!\n",
            "doc 130 is done!\n",
            "doc 131 is done!\n",
            "doc 132 is done!\n",
            "doc 133 is done!\n",
            "doc 134 is done!\n",
            "doc 135 is done!\n",
            "doc 136 is done!\n",
            "doc 137 is done!\n",
            "doc 138 is done!\n",
            "doc 139 is done!\n",
            "doc 140 is done!\n",
            "doc 141 is done!\n",
            "doc 142 is done!\n",
            "doc 143 is done!\n",
            "doc 144 is done!\n",
            "doc 145 is done!\n",
            "doc 146 is done!\n",
            "doc 147 is done!\n",
            "doc 148 is done!\n",
            "doc 149 is done!\n",
            "doc 150 is done!\n",
            "doc 151 is done!\n",
            "doc 152 is done!\n",
            "doc 153 is done!\n",
            "doc 154 is done!\n",
            "doc 155 is done!\n",
            "doc 156 is done!\n",
            "doc 157 is done!\n",
            "doc 158 is done!\n",
            "doc 159 is done!\n",
            "doc 160 is done!\n",
            "doc 161 is done!\n",
            "doc 162 is done!\n",
            "doc 163 is done!\n",
            "doc 164 is done!\n",
            "doc 165 is done!\n",
            "doc 166 is done!\n",
            "doc 167 is done!\n",
            "doc 168 is done!\n",
            "doc 169 is done!\n",
            "doc 170 is done!\n",
            "doc 171 is done!\n",
            "doc 172 is done!\n",
            "doc 173 is done!\n",
            "doc 174 is done!\n",
            "doc 175 is done!\n",
            "doc 176 is done!\n",
            "doc 177 is done!\n",
            "doc 178 is done!\n",
            "doc 179 is done!\n",
            "doc 180 is done!\n",
            "doc 181 is done!\n",
            "doc 182 is done!\n",
            "doc 183 is done!\n",
            "doc 184 is done!\n",
            "doc 185 is done!\n",
            "doc 186 is done!\n",
            "doc 187 is done!\n",
            "doc 188 is done!\n",
            "doc 189 is done!\n",
            "doc 190 is done!\n",
            "doc 191 is done!\n",
            "doc 192 is done!\n",
            "doc 193 is done!\n",
            "doc 194 is done!\n",
            "doc 195 is done!\n",
            "doc 196 is done!\n",
            "doc 197 is done!\n",
            "doc 198 is done!\n",
            "doc 199 is done!\n",
            "doc 200 is done!\n",
            "doc 201 is done!\n",
            "doc 202 is done!\n",
            "doc 203 is done!\n",
            "doc 204 is done!\n",
            "doc 205 is done!\n",
            "doc 206 is done!\n",
            "doc 207 is done!\n",
            "doc 208 is done!\n",
            "doc 209 is done!\n",
            "doc 210 is done!\n",
            "doc 211 is done!\n",
            "doc 212 is done!\n",
            "doc 213 is done!\n",
            "doc 214 is done!\n",
            "doc 215 is done!\n",
            "doc 216 is done!\n",
            "doc 217 is done!\n",
            "doc 218 is done!\n",
            "doc 219 is done!\n",
            "doc 220 is done!\n",
            "doc 221 is done!\n",
            "doc 222 is done!\n",
            "doc 223 is done!\n",
            "doc 224 is done!\n",
            "doc 225 is done!\n",
            "doc 226 is done!\n",
            "doc 227 is done!\n",
            "doc 228 is done!\n",
            "doc 229 is done!\n",
            "doc 230 is done!\n",
            "doc 231 is done!\n",
            "doc 232 is done!\n",
            "doc 233 is done!\n",
            "doc 234 is done!\n",
            "doc 235 is done!\n",
            "doc 236 is done!\n",
            "doc 237 is done!\n",
            "doc 238 is done!\n",
            "doc 239 is done!\n",
            "doc 240 is done!\n",
            "doc 241 is done!\n",
            "doc 242 is done!\n",
            "doc 243 is done!\n",
            "doc 244 is done!\n",
            "doc 245 is done!\n",
            "doc 246 is done!\n",
            "doc 247 is done!\n",
            "doc 248 is done!\n",
            "doc 249 is done!\n",
            "doc 250 is done!\n",
            "doc 251 is done!\n",
            "doc 252 is done!\n",
            "doc 253 is done!\n",
            "doc 254 is done!\n",
            "doc 255 is done!\n",
            "doc 256 is done!\n",
            "doc 257 is done!\n",
            "doc 258 is done!\n",
            "doc 259 is done!\n",
            "doc 260 is done!\n",
            "doc 261 is done!\n",
            "doc 262 is done!\n",
            "doc 263 is done!\n",
            "doc 264 is done!\n",
            "doc 265 is done!\n",
            "doc 266 is done!\n",
            "doc 267 is done!\n",
            "doc 268 is done!\n",
            "doc 269 is done!\n",
            "doc 270 is done!\n",
            "doc 271 is done!\n",
            "doc 272 is done!\n",
            "doc 273 is done!\n",
            "doc 274 is done!\n",
            "doc 275 is done!\n",
            "doc 276 is done!\n",
            "doc 277 is done!\n",
            "doc 278 is done!\n",
            "doc 279 is done!\n",
            "doc 280 is done!\n",
            "doc 281 is done!\n",
            "doc 282 is done!\n",
            "doc 283 is done!\n",
            "doc 284 is done!\n",
            "doc 285 is done!\n",
            "doc 286 is done!\n",
            "doc 287 is done!\n",
            "doc 288 is done!\n",
            "doc 289 is done!\n",
            "doc 290 is done!\n",
            "doc 291 is done!\n",
            "doc 292 is done!\n",
            "doc 293 is done!\n",
            "doc 294 is done!\n",
            "doc 295 is done!\n",
            "doc 296 is done!\n",
            "doc 297 is done!\n",
            "doc 298 is done!\n",
            "doc 299 is done!\n",
            "doc 300 is done!\n",
            "doc 301 is done!\n",
            "doc 302 is done!\n",
            "doc 303 is done!\n",
            "doc 304 is done!\n",
            "doc 305 is done!\n",
            "doc 306 is done!\n",
            "doc 307 is done!\n",
            "doc 308 is done!\n",
            "doc 309 is done!\n",
            "doc 310 is done!\n",
            "doc 311 is done!\n",
            "doc 312 is done!\n",
            "doc 313 is done!\n",
            "doc 314 is done!\n",
            "doc 315 is done!\n",
            "doc 316 is done!\n",
            "doc 317 is done!\n",
            "doc 318 is done!\n",
            "doc 319 is done!\n",
            "doc 320 is done!\n",
            "doc 321 is done!\n",
            "doc 322 is done!\n",
            "doc 323 is done!\n",
            "doc 324 is done!\n",
            "doc 325 is done!\n",
            "doc 326 is done!\n",
            "doc 327 is done!\n",
            "doc 328 is done!\n",
            "doc 329 is done!\n",
            "doc 330 is done!\n",
            "doc 331 is done!\n",
            "doc 332 is done!\n",
            "doc 333 is done!\n",
            "doc 334 is done!\n",
            "doc 335 is done!\n",
            "doc 336 is done!\n",
            "doc 337 is done!\n",
            "doc 338 is done!\n",
            "doc 339 is done!\n",
            "doc 340 is done!\n",
            "doc 341 is done!\n",
            "doc 342 is done!\n",
            "doc 343 is done!\n",
            "doc 344 is done!\n",
            "doc 345 is done!\n",
            "doc 346 is done!\n",
            "doc 347 is done!\n",
            "doc 348 is done!\n",
            "doc 349 is done!\n",
            "doc 350 is done!\n",
            "doc 351 is done!\n",
            "doc 352 is done!\n",
            "doc 353 is done!\n",
            "doc 354 is done!\n",
            "doc 355 is done!\n",
            "doc 356 is done!\n",
            "doc 357 is done!\n",
            "doc 358 is done!\n",
            "doc 359 is done!\n",
            "doc 360 is done!\n",
            "doc 361 is done!\n",
            "doc 362 is done!\n",
            "doc 363 is done!\n",
            "doc 364 is done!\n",
            "doc 365 is done!\n",
            "doc 366 is done!\n",
            "doc 367 is done!\n",
            "doc 368 is done!\n",
            "doc 369 is done!\n",
            "doc 370 is done!\n",
            "doc 371 is done!\n",
            "doc 372 is done!\n",
            "doc 373 is done!\n",
            "doc 374 is done!\n",
            "doc 375 is done!\n",
            "doc 376 is done!\n",
            "doc 377 is done!\n",
            "doc 378 is done!\n",
            "doc 379 is done!\n",
            "doc 380 is done!\n",
            "doc 381 is done!\n",
            "doc 382 is done!\n",
            "doc 383 is done!\n",
            "doc 384 is done!\n",
            "doc 385 is done!\n",
            "doc 386 is done!\n",
            "doc 387 is done!\n",
            "doc 388 is done!\n",
            "doc 389 is done!\n",
            "doc 390 is done!\n",
            "doc 391 is done!\n",
            "doc 392 is done!\n",
            "doc 393 is done!\n",
            "doc 394 is done!\n",
            "doc 395 is done!\n",
            "doc 396 is done!\n",
            "doc 397 is done!\n",
            "doc 398 is done!\n",
            "doc 399 is done!\n",
            "doc 400 is done!\n",
            "doc 401 is done!\n",
            "doc 402 is done!\n",
            "doc 403 is done!\n",
            "doc 404 is done!\n",
            "doc 405 is done!\n",
            "doc 406 is done!\n",
            "doc 407 is done!\n",
            "doc 408 is done!\n",
            "doc 409 is done!\n",
            "doc 410 is done!\n",
            "doc 411 is done!\n",
            "doc 412 is done!\n",
            "doc 413 is done!\n",
            "doc 414 is done!\n",
            "doc 415 is done!\n",
            "doc 416 is done!\n",
            "doc 417 is done!\n",
            "doc 418 is done!\n",
            "doc 419 is done!\n",
            "doc 420 is done!\n",
            "doc 421 is done!\n",
            "doc 422 is done!\n",
            "doc 423 is done!\n",
            "doc 424 is done!\n",
            "doc 425 is done!\n",
            "doc 426 is done!\n",
            "doc 427 is done!\n",
            "doc 428 is done!\n",
            "doc 429 is done!\n",
            "doc 430 is done!\n",
            "doc 431 is done!\n",
            "doc 432 is done!\n",
            "doc 433 is done!\n",
            "doc 434 is done!\n",
            "doc 435 is done!\n",
            "doc 436 is done!\n",
            "doc 437 is done!\n",
            "doc 438 is done!\n",
            "doc 439 is done!\n",
            "doc 440 is done!\n",
            "doc 441 is done!\n",
            "doc 442 is done!\n",
            "doc 443 is done!\n",
            "doc 444 is done!\n",
            "doc 445 is done!\n",
            "doc 446 is done!\n",
            "doc 447 is done!\n",
            "doc 448 is done!\n",
            "doc 449 is done!\n",
            "doc 450 is done!\n",
            "doc 451 is done!\n",
            "doc 452 is done!\n",
            "doc 453 is done!\n",
            "doc 454 is done!\n",
            "doc 455 is done!\n",
            "doc 456 is done!\n",
            "doc 457 is done!\n",
            "doc 458 is done!\n",
            "doc 459 is done!\n",
            "doc 460 is done!\n",
            "doc 461 is done!\n",
            "doc 462 is done!\n",
            "doc 463 is done!\n",
            "doc 464 is done!\n",
            "doc 465 is done!\n",
            "doc 466 is done!\n",
            "doc 467 is done!\n",
            "doc 468 is done!\n",
            "doc 469 is done!\n",
            "doc 470 is done!\n",
            "doc 471 is done!\n",
            "doc 472 is done!\n",
            "doc 473 is done!\n",
            "doc 474 is done!\n",
            "doc 475 is done!\n",
            "doc 476 is done!\n",
            "doc 477 is done!\n",
            "doc 478 is done!\n",
            "doc 479 is done!\n",
            "doc 480 is done!\n",
            "doc 481 is done!\n",
            "doc 482 is done!\n",
            "doc 483 is done!\n",
            "doc 484 is done!\n",
            "doc 485 is done!\n",
            "doc 486 is done!\n",
            "doc 487 is done!\n",
            "doc 488 is done!\n",
            "doc 489 is done!\n",
            "doc 490 is done!\n",
            "doc 491 is done!\n",
            "doc 492 is done!\n",
            "doc 493 is done!\n",
            "doc 494 is done!\n",
            "doc 495 is done!\n",
            "doc 496 is done!\n",
            "doc 497 is done!\n",
            "doc 498 is done!\n",
            "doc 499 is done!\n"
          ]
        }
      ]
    }
  ]
}